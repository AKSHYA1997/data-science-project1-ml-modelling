{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06532205",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-84087e996fa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "import os, re, csv, string\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn import metrics, preprocessing, cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "611d0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d69b8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(file_name, new_file_name):\n",
    "    rf_file = open(file_name, 'r',encoding='utf-8')\n",
    "    wf_file = open(new_file_name, 'w')\n",
    "    count = 0\n",
    "    for line in rf_file:\n",
    "        count += 1\n",
    "        if count % 100000 == 0:\n",
    "            print(count)\n",
    "        wf_file.write(line.replace('\\x00', ''))\n",
    "    print(count)\n",
    "    rf_file.close()\n",
    "    wf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21e7a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(filename, delim):\n",
    "    csv.field_size_limit(99999999)\n",
    "    rf = open(filename, 'rt', encoding='utf-8')\n",
    "    rf_csv = csv.reader(rf, delimiter = delim, quotechar = '\"')\n",
    "    count_dic = {}\n",
    "    count = 0\n",
    "    for project in rf_csv:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Lines read in essays.csv: \" + str(count))\n",
    "        if project[0].strip() not in count_dic:\n",
    "            title_token = WhitespaceTokenizer().tokenize(project[2].strip())\n",
    "            short_description_token = WhitespaceTokenizer().tokenize(project[3].strip())\n",
    "            need_statement_token = WhitespaceTokenizer().tokenize(project[4].strip())\n",
    "            essay_token = WhitespaceTokenizer().tokenize(project[5].strip())\n",
    "            count_dic[project[0].strip()] = (len(title_token), len(short_description_token), len(need_statement_token), len(essay_token))\n",
    "    print(\"Lines read in essays.csv: \" + str(count))\n",
    "    rf.close()\n",
    "    return count_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "513097c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rslt(count_dic,folder):\n",
    "    fn = \"C:/Users/HP/Desktop/project-1/count.csv\"\n",
    "    wf = open(fn, 'w')\n",
    "    wf.write('projectid,title_cnt,short_description_cnt,need_statement_cnt,essay_cnt\\n')\n",
    "    for project in count_dic.keys():\n",
    "        wf.write(project + ',' + str(count_dic[project][0]) + ',' + str(count_dic[project][1]) + ',' + str(count_dic[project][2]) + ',' + str(count_dic[project][3]) + '\\n')\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3171775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_projectlist(data):\n",
    "    data = open(data, 'r',encoding='utf-8')\n",
    "    data_csv = csv.reader(data, delimiter = ',', quotechar = '\"')\n",
    "    project_dic = {}\n",
    "    heading = 1\n",
    "    for line in data_csv:\n",
    "        if heading:\n",
    "            heading = 0\n",
    "        else:\n",
    "            project_dic[line[0]] = [line[1], line[2], line[3]]\n",
    "    data.close()\n",
    "    return project_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25e6a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37d14bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_essays(data, project_dic):\n",
    "    data = open(data, 'r',encoding='utf-8')\n",
    "    data_csv = csv.reader(data, delimiter = ',', quotechar = '\"')\n",
    "    project_dic_train = {}\n",
    "    project_dic_validation = {}\n",
    "    project_dic_test = {}\n",
    "    heading = 1\n",
    "    count = 0\n",
    "    count_train = 0\n",
    "    count_validation = 0\n",
    "    count_test = 0\n",
    "    for line in data_csv:\n",
    "        count += 1\n",
    "        if count % 100000 == 0:\n",
    "            print(count)\n",
    "        if heading:\n",
    "            heading = 0\n",
    "        else:\n",
    "            if line[0] in project_dic:\n",
    "                if project_dic[line[0]][0] == '0':\n",
    "                    project_dic_train[line[0]] = [line[2], line[3], line[4], line[5], project_dic[line[0]][2]]\n",
    "                    count_train += 1\n",
    "                else:\n",
    "                    if project_dic[line[0]][1] == '0':\n",
    "                        project_dic_validation[line[0]] = [line[2], line[3], line[4], line[5], project_dic[line[0]][2]]\n",
    "                        count_validation += 1\n",
    "                    else:\n",
    "                        project_dic_test[line[0]] = [line[2], line[3], line[4], line[5]]\n",
    "                        count_test += 1\n",
    "    print(count)\n",
    "    print(\"Train Count: \", count_train)\n",
    "    print(\"Validation Count: \", count_validation)\n",
    "    print(\"Test Count: \", count_test)\n",
    "    data.close()\n",
    "\n",
    "    title_train = []\n",
    "    short_description_train = []\n",
    "    need_statement_train = []\n",
    "    essay_train = []\n",
    "    y_train = []\n",
    "    project_train = []\n",
    "    count_train = 0\n",
    "    for project in project_dic_train.keys():\n",
    "        count_train += 1\n",
    "        project_train.append(project)\n",
    "        y_train.append(int(project_dic_train[project][4]))\n",
    "        title_train.append(project_dic_train[project][0])\n",
    "        short_description_train.append(project_dic_train[project][1])\n",
    "        need_statement_train.append(project_dic_train[project][2])\n",
    "        essay_train.append(project_dic_train[project][3])\n",
    "    print(\"Train Count: \", count_train)\n",
    "\n",
    "    title_validation = []\n",
    "    short_description_validation = []\n",
    "    need_statement_validation = []\n",
    "    essay_validation = []\n",
    "    y_validation = []\n",
    "    project_validation = []\n",
    "    count_validation = 0\n",
    "    for project in project_dic_validation.keys():\n",
    "        count_validation += 1\n",
    "        project_validation.append(project)\n",
    "        y_validation.append(int(project_dic_validation[project][4]))\n",
    "        title_validation.append(project_dic_validation[project][0])\n",
    "        short_description_validation.append(project_dic_validation[project][1])\n",
    "        need_statement_validation.append(project_dic_validation[project][2])\n",
    "        essay_validation.append(project_dic_validation[project][3])\n",
    "    print(\"Validation Count: \", count_validation)\n",
    "\n",
    "    title_test = []\n",
    "    short_description_test = []\n",
    "    need_statement_test = []\n",
    "    essay_test = []\n",
    "    project_test = []\n",
    "    count_test = 0\n",
    "    for project in project_dic_test.keys():\n",
    "        count_test += 1\n",
    "        project_test.append(project)\n",
    "        title_test.append(project_dic_test[project][0])\n",
    "        short_description_test.append(project_dic_test[project][1])\n",
    "        need_statement_test.append(project_dic_test[project][2])\n",
    "        essay_test.append(project_dic_test[project][3])\n",
    "    print(\"Test Count: \", count_test)\n",
    "\n",
    "    return np.asarray(y_train), np.asarray(y_validation), np.asarray(project_train), np.asarray(project_validation), np.asarray(project_test), title_train, title_validation, title_test, short_description_train, short_description_validation, short_description_test, need_statement_train, need_statement_validation, need_statement_test, essay_train, essay_validation, essay_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e00ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d0907e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_train, y_validation, text_train, text_validation, text_test, project_train, project_validation, project_test, ngrams_max, freq_min, sgd_alpha, sgd_iter, rand_seed):\n",
    "    tfv = TfidfVectorizer(min_df=freq_min, max_features=None, decode_error = 'ignore', strip_accents='unicode', analyzer='word', token_pattern=r'[a-zA-Z]{1,}', ngram_range=(1, ngrams_max), use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "    rand_state = np.random.RandomState(seed=rand_seed)\n",
    "    sgd = SGDClassifier(loss=\"log\", penalty=\"l2\", shuffle=True, verbose=0, max_iter=sgd_iter, fit_intercept=True, alpha=sgd_alpha)\n",
    "\n",
    "    rand = np.ravel(rand_state.rand(len(y_train),1))\n",
    "    group = []\n",
    "    for number in rand:\n",
    "        if number > 0.9:\n",
    "            group.append(1)\n",
    "        elif number > 0.8:\n",
    "            group.append(2)\n",
    "        elif number > 0.7:\n",
    "            group.append(3)\n",
    "        elif number > 0.6:\n",
    "            group.append(4)\n",
    "        elif number > 0.5:\n",
    "            group.append(5)\n",
    "        elif number > 0.4:\n",
    "            group.append(6)\n",
    "        elif number > 0.3:\n",
    "            group.append(7)\n",
    "        elif number > 0.2:\n",
    "            group.append(8)\n",
    "        elif number > 0.1:\n",
    "            group.append(9)\n",
    "        else:\n",
    "            group.append(10)\n",
    "    for i in range(1,11):\n",
    "        text_train_tmp = []\n",
    "        text_test_tmp = []\n",
    "        y_train_tmp = []\n",
    "        y_test_tmp = []\n",
    "        project_train_tmp = []\n",
    "        project_test_tmp = []\n",
    "        for j in range(0,len(group)):\n",
    "            if group[j] == i:\n",
    "                text_test_tmp.append(text_train[j])\n",
    "                y_test_tmp.append(y_train[j])\n",
    "                project_test_tmp.append(project_train[j])\n",
    "            else:\n",
    "                text_train_tmp.append(text_train[j])\n",
    "                y_train_tmp.append(y_train[j])\n",
    "                project_train_tmp.append(project_train[j])\n",
    "        project_test_tmp = np.asarray(project_test_tmp)\n",
    "        print(text_train_tmp)\n",
    "        tfv.fit(text_train_tmp)\n",
    "        x_train = tfv.transform(text_train_tmp)\n",
    "        sgd.fit(x_train,y_train_tmp)\n",
    "        x_test = tfv.transform(text_test_tmp)\n",
    "        prob = sgd.predict_proba(x_test)[:,1]\n",
    "        if i == 1:\n",
    "            result_train = np.vstack((project_test_tmp,prob))\n",
    "            print(\"Group: \", i)\n",
    "        else:\n",
    "            tmp = np.vstack((project_test_tmp,prob))\n",
    "            result_train = np.hstack((result_train,tmp))\n",
    "            print(\"Group: \", i)\n",
    "\n",
    "    tfv.fit(text_train)\n",
    "    x_train = tfv.transform(text_train)\n",
    "    sgd.fit(x_train,y_train)\n",
    "    x_validation = tfv.transform(text_validation)\n",
    "    prob = sgd.predict_proba(x_validation)[:,1]\n",
    "    result_validation = np.vstack((project_validation,prob))\n",
    "\n",
    "    text_train_validation = text_train + text_validation\n",
    "    y_train_validation = np.hstack((y_train, y_validation))\n",
    "    tfv.fit(text_train_validation)\n",
    "    x_train_validation = tfv.transform(text_train_validation)\n",
    "    sgd.fit(x_train_validation, y_train_validation)\n",
    "    x_test = tfv.transform(text_test)\n",
    "    prob = sgd.predict_proba(x_test)[:,1]\n",
    "    result_test = np.vstack((project_test, prob))\n",
    "\n",
    "    result = np.hstack((result_train, result_validation, result_test))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8eccd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result, filename, varname):\n",
    "    wfile = open(filename, \"w\")\n",
    "    wfile.write(\"projectid,\"+varname+\"\\n\")\n",
    "    for i in range(0,len(result[0])):\n",
    "        wfile.write(result[0][i]+\",\"+result[1][i]+\"\\n\")\n",
    "    wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dd787d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "87881ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines read in essays.csv: 10000\n",
      "Lines read in essays.csv: 20000\n",
      "Lines read in essays.csv: 30000\n",
      "Lines read in essays.csv: 40000\n",
      "Lines read in essays.csv: 50000\n",
      "Lines read in essays.csv: 60000\n",
      "Lines read in essays.csv: 70000\n",
      "Lines read in essays.csv: 80000\n",
      "Lines read in essays.csv: 90000\n",
      "Lines read in essays.csv: 100000\n",
      "Lines read in essays.csv: 110000\n",
      "Lines read in essays.csv: 120000\n",
      "Lines read in essays.csv: 130000\n",
      "Lines read in essays.csv: 140000\n",
      "Lines read in essays.csv: 150000\n",
      "Lines read in essays.csv: 160000\n",
      "Lines read in essays.csv: 170000\n",
      "Lines read in essays.csv: 180000\n",
      "Lines read in essays.csv: 190000\n",
      "Lines read in essays.csv: 200000\n",
      "Lines read in essays.csv: 210000\n",
      "Lines read in essays.csv: 220000\n",
      "Lines read in essays.csv: 230000\n",
      "Lines read in essays.csv: 240000\n",
      "Lines read in essays.csv: 241539\n",
      "100000\n",
      "200000\n",
      "241539\n",
      "Train Count:  0\n",
      "Validation Count:  0\n",
      "Test Count:  241538\n",
      "Train Count:  0\n",
      "Validation Count:  0\n",
      "Test Count:  241538\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-c281ed67aae9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshort_description_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshort_description_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshort_description_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_statement_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_statement_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_statement_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0messay_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0messay_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0messay_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_essays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/HP/Desktop/project-1/essay_sample-1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_dic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mresult_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngrams_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd_alpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3165832\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mresult_short_description\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshort_description_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshort_description_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshort_description_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngrams_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd_alpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2105827\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mresult_need_statement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_statement_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_statement_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_statement_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngrams_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd_alpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m7392409\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-519a108a2da0>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(y_train, y_validation, text_train, text_validation, text_test, project_train, project_validation, project_test, ngrams_max, freq_min, sgd_alpha, sgd_iter, rand_seed)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mproject_test_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject_test_tmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_train_tmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mtfv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_train_tmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_train_tmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_tmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\data_science\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1825\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1826\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1827\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1828\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\data_science\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\data_science\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1132\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m   1135\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":    \n",
    "    folder = sys.argv[1]\n",
    "    csv.field_size_limit(999999999)\n",
    "    #clean_file(\"essays.csv\",\"essays_v2.csv\")\n",
    "    count_dic = count(\"C:/Users/HP/Desktop/project-1/essay_sample-1.csv\",',')\n",
    "    print_rslt(count_dic,folder)\n",
    "    \n",
    "    project_dic = import_projectlist(\"C:/Users/HP/Desktop/project-1/projects_sample.csv\")\n",
    "    y_train, y_validation, project_train, project_validation, project_test, title_train, title_validation, title_test, short_description_train, short_description_validation, short_description_test, need_statement_train, need_statement_validation, need_statement_test, essay_train, essay_validation, essay_test = read_essays(\"C:/Users/HP/Desktop/project-1/essay_sample-1.csv\", project_dic)\n",
    "    \n",
    "    result_title = score(y_train, y_validation, title_train, title_validation, title_test, project_train, project_validation, project_test, ngrams_max = 3, freq_min = 10, sgd_alpha = 0.00005, sgd_iter = 20, rand_seed = 3165832)\n",
    "    result_short_description = score(y_train, y_validation, short_description_train, short_description_validation, short_description_test, project_train, project_validation, project_test, ngrams_max = 2, freq_min = 10, sgd_alpha = 0.00001, sgd_iter = 20, rand_seed = 2105827)\n",
    "    result_need_statement = score(y_train, y_validation, need_statement_train, need_statement_validation, need_statement_test, project_train, project_validation, project_test, ngrams_max = 2, freq_min = 10, sgd_alpha = 0.00005, sgd_iter = 20, rand_seed = 7392409)\n",
    "    result_essay = score(y_train, y_validation, essay_train, essay_validation, essay_test, project_train, project_validation, project_test, ngrams_max = 4, freq_min = 2, sgd_alpha = 0.000004, sgd_iter = 20, rand_seed = 3866942)\n",
    "    \n",
    "    print_result(result_title, \"result_title.csv\",\"title_tfidf\")\n",
    "    print_result(result_short_description, \"result_short_description.csv\",\"short_description_tfidf\")\n",
    "    print_result(result_need_statement, \"result_need_statement.csv\",\"need_statement_tfidf\")\n",
    "    print_result(result_essay, \"result_essay_20140710.csv\",\"essay_tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30075aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
